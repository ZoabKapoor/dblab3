Exercise 1: Life of a query in SimpleDB

Step 1: simpledb.Parser.main() and simpledb.Parser.start()

simpledb.Parser.main() is the entry point for the SimpleDB system. It calls simpledb.Parser.start(). The latter performs three main actions:

1. It populates the SimpleDB catalog from the catalog text file provided by the user as argument (Database.getCatalog().loadSchema(argv[0]);).
2. For each table defined in the system catalog, it computes statistics over the data in the table by calling: TableStats.computeStatistics(), which instantiates a new TableStats: TableStats s = new TableStats(tableid, IOCOSTPERPAGE);
3. It processes the statements submitted by the user (processNextStatement(new ByteArrayInputStream(statementBytes));)

Step 2: simpledb.Parser.processNextStatement()

This method takes two key actions:

First, it gets a physical plan for the query by invoking handleQueryStatement((ZQuery)s);
Then it executes the query by calling query.execute();

Step 3: simpledb.Parser.handleQueryStatement()

This method does 4 main things:

First, it creates a new query object with the transactionId passed in.
Then, it creates a logical plan for the query by calling parseQueryLogicalPlan(tId, s);
It then converts the logical plan into a physical plan (represented by a DbIterator) by calling lp.physicalPlan(), passing it the transactionId of the transaction, the explain flag (which determines whether to print the final join ordering for the user's benefit), and the map containing the statistics for the table. 
Finally, it sets the logical plan and physical plan of the query to the results above and (after some metaprogramming) returns the query object. 

Step 4: simpledb.LogicalPlan.physicalPlan()

This function does a lot! We'll mention the key operations of the function below.
First, it iterates over the tables included in the query, gets its associated TableStats object from baseTableStats (which is passed in), and stores the TableStats in a HashMap. 
Then, it iterates over an array LogicalFilterNodes created by parseQueryLogicalPlan() and merges the selectivity of that filter into an accumulator of selectivities using TableStats.estimateSelectivity(). In particular, it initially sets the accumulator to 1 (which is what the selectivity would be if there were no filters), and with additional filter it multiplies the selectivity of that filter with the value of the accumulator
Then, it creates a JoinOptimizer and calls its orderJoins() method to order joins optimally and then links the joins in subplanMap to create a DbIterator node that iterates over the joins in the right order. 
Then, it walks through the select list to determine the order in which to project output fields. It checks for aggregation and groupBy and instantiates appropriate DbIterators (Aggregate/OrderBy) if necessary (which we implemeneted in lab2). 
Finally, it returns a project DbIterator that projects the result of the physical plan onto the fields we care about.

Step 5a: simpledb.TableStats.estimateSelectivity()

We'll be implementing this function in this lab! It estimates the selectivity of a particular predicate on a particular field within a particular relation, using the histogram associated with that field stored in the TableStats.

Step 5b: simpleDb.JoinOptimizer.orderJoins()

We'll be implementing this function in this lab as well! It takes in the statistics for each table involved in the join, and the selectivities of the filter predicates on each table in the join, and returns joins, which is a Vector<LogicalJoinNode> that stores LogicalJoinNodes in the left-deep order of their execution. It will do this by estimating the cost of every subplan of joins, starting with subplans of size 1 and working its way up to subplans of size n (where n is the size of the returned Vector<LogicalJoinNode>). It will then pick the plan of size n with the lowest cost and order joins on the schematic of that plan.


Writeup:

Code completeness and time spent:

All exercises are complete, and the implementation passes all tests! I probably spent about 18 hours on this lab, which isn't so bad compared to the past two. 

Issues I ran into:

1. One thing that was particularly difficult was writing the IntHistogram correctly. Coding the histogram so that it could handle ranges that were smaller or greater than numBuckets was tough, as was dealing with non-integer bucketWidths (after speaking to Prof Beth, I recognize that there is a way to force your bucketWidths to all be integers, but I chose to divide up the range given into numBuckets equally sized subpieces, and mapped integer values into the bucket whose range they fit in). In fact, after initially "successfully" implementing IntHistogram, I had to come back to it twice and modify the code because the TableStats or JoinOptimizer tests revealed an implementation flaw. I'm not particularly happy that my final implementation basically has an if case for whether we're dealing with a situation where numBuckets >= range or numBuckets < range, but it works and seems logically sound, and reworking it doesn't seem like it'll be worth it. 
2. The fact that there was no general Histogram interface that we could use in TableStats meant that I had to make my Histogram array an array[Object] and use casting to get the behaviour I wanted. A Histogram interface  would've been a nice thing to have, but then there would be the problem of making addValue be able to take an int or a string. I suppose you could've had addValue(Object), but then you'd run back into casting and it would still be inelegant.

API changes:

Unlike in previous labs, I didn't change the public API of any of the classes I was dealing with at all (I think). I did add a bunch of private helper methods into IntHistogram, but since these are private they don't affect the APIs provided. I also created a private class called Range in TableStats to help with the TableStats constructor. 